# 개요 
[1.요구사항 분석](#1.요구사항분석) <br>
[2.데이터 수집](#데이터수집-획득Lv2) <br>
[2.데이터 적재](#데이터-적재) <br>

1. 요구사항 분석
2. 데이터 수집
    - 난이도에 따라 구성 분리
        - Lv1
            - 데이터를 제공받는 케이스
                - 공공데이터, 공모전 데이터(활용 한계), 연구기관 / 교육기관 제공
                - 사내 데이터 (회사 내부 데이터)
        - Lv2
            - open API가 존재하는 케이스
            - 인증키를 통해서, 하루에 적정량의 데이터를 질의하여 활용할수 있는 경우 
            - 데이터는 통상 JSON / XML로 제공 => 반정형 데이터
            <br>

        --------------이하는 웹에서 구할경우------------------------------    

        - Lv3
            - 해당 웹페이지에서 바로 데이터를 수집할 수 있다면?
            - Web Scarpping (웹 스크래핑)
            - request, bs4(beatiful soup)
            - 비정형 데이터 (날것의 데이터, 구조가 없는 데이터)
        - Lv4
            - 해당 웹페이지가 사용자와 인터렉션을 통해서(반응해서) 데이터가 노출된다
            - 더보기, 스크롤, 로그인, 검색 등등 케이스. ajax를 사용한 사이트
            - selenium(셀레니움) + 웹드라이버 (브라우저 회사별로 제공하는)
        
            - 자동화
                * os 레벨에서 자동으로 데이터를 수집하게 하는 활동을 작성 / 운용
                * Lv3/Lv4 같은 경우는 단시간에 빠른 접속을 지속적으로 시도하면 디도스로 간주 할 수 있기 때문에, 적절한 시간 조절
                    * 고급(접속한 유저의 ip를 우회하여(플락시 서버 활용) 처리)
            
                <br>
            
    
3. 데이터 준비 / 전처리 /적제
    - 전처리, 정제, 적제
    - 이상치, 결측치 처리
<br>

4. 데이터 분석
    - EDA (탐색적 분석)
    - 인과분석
    - ...
<br>

5. 모델 구축
    - 통계 모델(모형)
    - 머신러닝 모델
    - 딥러닝 모델
6. 시스템 구축 / 서비스 구성 / 레포트 => 산출물
<br>


# 데이터수집 획득Lv2
## 사용기술
    - open API 필요
        * dev.naver.com or dev.kakao.com
    - Client ID : ... <br>
    - Client PW : ...
    - API 문서 : https://openapi.naver.com/v1/search/news
    - 검색 URL (응답 데이터 json) : https://openapi.naver.com/v1/search/news.json

    - 예시

```
curl "https://openapi.naver.com/v1/search/news.xml?query=%EC%A3%BC%EC%8B%9D&display=10&start=1&sort=sim" \
-H "X-Naver-Client-Id: {애플리케이션 등록 시 발급받은 client id 값}" \
-H "X-Naver-Client-Secret: {애플리케이션 등록 시 발급받은 client secret 값}" -v
```
    - request 모듈 필요 -> 통신수행(http)
        * get방식을 주로 사용
        * 개인별 인증키는 헤더에 숨겨서 전송

## 구현
    1. 필요한 모듈 가져오기
    2. 환경변수, 통신에 필요한 키를 정의
    3. URL 정의 (JSON)
    4. 파라미터 정의 (검색어가 한글인 경우, uft-8 인코딩 처리)
    5. 최종 GET방식으로 요청하는 url
    6. 통신 객체 생성
    7. 클라이언트 키, 보안키 생성
    8. 통신
    9. 응답코드 확인 -> 
        - 200 : 정상을 의미
        - 401 : 권한오류
        - 404 : 해당 페이지가 없다
        - 500 : 서버 내부 에러
    10. 응답 데이터를 JSON 객체로 로드 -> 딕셔너리, 리스트 조합 구성
    11. 파싱

# 데이터 적재
- 파일
    - csv, xlsx

- 데이터베이스
    - RDBMS
        - 기업형 DB, ms-sql, oracle
        - 현재 작업 기준에서는 코랩에서 작동시, 고정 IP나 도메인을 가진 데이터 베이스를 활용하여 저장해야한다.
            - aws
    - No-SQL
        - 몽고디비 -> 로그 저장

- 절차
    - 데이터 구조 : [ {}, {}, {}, ...] 준비
    - pandas를 이용하여 DataFrame 구성
    - pymysql + sqlalchemy 이용하여 접속
    - 데이터를 데이터베이스에 적재
    - 연결 종료